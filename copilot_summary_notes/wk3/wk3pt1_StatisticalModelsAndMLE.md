Here is the updated document with all instances of improperly formatted symbols replaced with their proper mathematical symbols:

---

## **Week 3 Notes: Statistical Models and Maximum Likelihood Estimation**

### **1. Overview**
- **Statistical Models**: These are frameworks that describe relationships between variables using probability distributions. They help generalize findings from data and identify trends or dependencies.
- **Key Idea**: When we assume a specific distribution (e.g., Gaussian), we can use parameters like mean (ğœ‡) and variance (ğœÂ²) to characterize the data.
- **Example**: If patient ages follow a normal distribution, we can use the mean and variance to summarize the data and perform tests like the t-test.

---

### **2. Statistical Models**
- **Definition**: A statistical model is a family of distributions or relationships between variables with unknown parameters determined from data.
- **Parametric Models**: These assume a specific functional form (e.g., Gaussian distribution) but leave parameters (e.g., mean, variance) to be estimated from data.
  
#### **Example: Gaussian Distribution**
- The probability density function for a Gaussian distribution is:
  ```markdown
  ğ‘“(ğ‘¥|ğœ‡, ğœÂ²) = 1 / âˆš(2ğœ‹ğœÂ²) ğ‘’^(-(ğ‘¥âˆ’ğœ‡)Â² / 2ğœÂ²)
  ```
  - ğœ‡: Mean
  - ğœÂ²: Variance

- **Application**: If gene expression levels follow a Gaussian distribution, we can use this model to predict outcomes or test hypotheses.

---

### **3. Statistical Independence**
- **Definition**: Two random variables ğ‘‹ and ğ‘Œ are independent if knowing ğ‘‹ provides no information about ğ‘Œ:
  ```markdown
  ğ‘ƒ(ğ‘Œ|ğ‘‹) = ğ‘ƒ(ğ‘Œ)
  ```

#### **Example in R**:
```R
# Simulate independent variables
x <- rnorm(10000, mean = 10, sd = 3)
y <- rnorm(10000, mean = 10, sd = 3)

# Plot histograms and scatterplot
hist(x, main = "Distribution of X")
hist(y, main = "Distribution of Y")
plot(x, y, main = "Scatterplot of X vs Y")
```
- **Observation**: The scatterplot shows no relationship between ğ‘‹ and ğ‘Œ, confirming independence.

---

### **4. Linear Models**
- **Definition**: A linear model describes the relationship between a predictor variable ğ‘‹ and a response variable ğ‘Œ:
  ```markdown
  ğ‘Œ = ğ›½â‚€ + ğ›½â‚ğ‘‹ + ğœ€
  ```
  - ğ›½â‚€: Intercept
  - ğ›½â‚: Slope
  - ğœ€: Error term (unexplained variation)

#### **Key Concepts**:
- **Residuals**: The difference between observed and predicted values:
  ```markdown
  ğ‘Ÿáµ¢ = ğ‘¦áµ¢ âˆ’ (ğ›½â‚€ + ğ›½â‚ğ‘¥áµ¢)
  ```
- **Least Squares**: The most common method to fit a linear model by minimizing the sum of squared residuals:
  ```markdown
  âˆ‘áµ¢â‚Œâ‚â¿ ğ‘Ÿáµ¢Â²
  ```

#### **Example in R**:
```R
# Simulate data
x <- rnorm(100, mean = 10, sd = 3)
y <- 2 * x + 5 + rnorm(100, sd = 2)

# Fit linear model
model <- lm(y ~ x)
summary(model)

# Plot data and fitted line
plot(x, y, main = "Linear Model Fit")
abline(model, col = "red")
```

---

### **5. Maximum Likelihood Estimation (MLE)**
- **Definition**: MLE estimates model parameters by maximizing the likelihood of the observed data under the model.
- **Likelihood Function**: The probability of observing the data given the parameters:
  ```markdown
  ğ¿(ğœƒ|ğ‘¥â‚, ğ‘¦â‚; â€¦; ğ‘¥â‚™, ğ‘¦â‚™) = âˆáµ¢â‚Œâ‚â¿ ğ‘ƒ(ğ‘¥áµ¢, ğ‘¦áµ¢|ğœƒ)
  ```
  - ğœƒ: Parameters (e.g., ğ‘, ğ‘ in a linear model)

#### **Log-Likelihood for Linear Models**:
- Assuming Gaussian noise, the log-likelihood is:
  ```markdown
  log ğ¿ = âˆ’(ğ‘› / 2) log(2ğœ‹ğœÂ²) âˆ’ (1 / 2ğœÂ²) âˆ‘áµ¢â‚Œâ‚â¿ (ğ‘¦áµ¢ âˆ’ ğ‘ğ‘¥áµ¢ âˆ’ ğ‘)Â²
  ```
  - The second term is the sum of squared residuals.

#### **Key Insight**:
- Maximizing the log-likelihood is equivalent to minimizing the sum of squared residuals (least squares method).

#### **Steps to Perform MLE**:
1. Assume a model (e.g., ğ‘¦ = ğ‘ğ‘¥ + ğ‘ + ğœ€).
2. Compute the likelihood or log-likelihood.
3. Differentiate with respect to parameters (ğ‘, ğ‘) and solve for the maximum.

---

### **6. Connection Between MLE and Least Squares**
- **Key Insight**: Least squares estimates are MLE estimates under the assumption of Gaussian noise.
- **Why This Matters**: This connection provides a statistical justification for using least squares in linear regression.

---

### **7. Practical Application**
- **Real-World Example**: Predicting patient blood pressure (ğ‘Œ) based on age (ğ‘‹).
  - Fit a linear model: ğ‘Œ = ğ›½â‚€ + ğ›½â‚ğ‘‹ + ğœ€.
  - Use MLE to estimate ğ›½â‚€ and ğ›½â‚.
  - Analyze residuals to validate the model.

---

### **8. Residual Analysis**
- **Purpose**: To assess the goodness of fit and validate model assumptions (e.g., normality, constant variance).
- **Steps**:
  1. Calculate residuals: ğ‘Ÿáµ¢ = ğ‘¦áµ¢ âˆ’ ğ‘¦â€²áµ¢.
  2. Plot residuals vs. fitted values.
  3. Check for patterns (e.g., non-random patterns indicate model issues).

---

### **Summary**
- **Statistical Models**: Frameworks for understanding data relationships.
- **Linear Models**: Simple yet powerful tools for quantifying dependencies.
- **MLE**: A robust method for parameter estimation, closely related to least squares.
- **Practical Applications**: Widely used in biomedical research for prediction and hypothesis testing.

---

Let me know if you need further adjustments or additional examples!